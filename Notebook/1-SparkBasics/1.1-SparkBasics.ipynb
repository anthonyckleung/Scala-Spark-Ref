{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Spark Scala running in a Jupyter notebook, one simply initializes it with the `spark` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.19:4041\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1588616857471)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@76bdbbaa\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Context\n",
    "\n",
    "In Spark compiled jobs (like in a script), we start by creating a **Spark Context** object named `sc`. The logic is illustrated as follows:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "\n",
    "val spark = SparkSession.builder.                // Use the builder pattern\n",
    "            master(\"local[*]\").                  // Run locally on all cores\n",
    "            appName(\"MyApp\").                    // Name of your app\n",
    "            config(\"spark.app.id\", \"Console\").   // Silence Metric Warnings\n",
    "            getOrCreate()                        // Create it\n",
    "\n",
    "val sc = spark.sparkContext                      // Extract SparkContext\n",
    "```\n",
    "\n",
    "**Only 1 SparkContext at a time!**\n",
    "* Spark is designed for single user\n",
    "* Only one sparkContext per program/notebook.\n",
    "* Before starting a new sparkContext. Stop the one currently running with `sc.stop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic RDD commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize\n",
    "\n",
    "* Simplest way to create an RDD\n",
    "* The method `A = sc.parallelize(L)`, creates an RDD named `A` from list `L`.\n",
    "* `A` is an RDD of type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: List[scala.collection.immutable.Range.Inclusive] = List(Range(1, 2, 3))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1 to 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[2] at parallelize at <console>:27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "A: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:27\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val A = sc.parallelize(List.range(0, 3))\n",
    "println(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect\n",
    "\n",
    "* RDD content is distributed among all executors.\n",
    "* `collect()` is the inverse of `parallelize()`.\n",
    "* Collects the elements of the RDD.\n",
    "* Returns a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I@26678e55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "L: Array[Int] = Array(0, 1, 2)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val L = A.collect()\n",
    "println(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `.collect()` eliminates the benefits of parallelism**\n",
    "\n",
    "It is often tempting to `.collect()` and RDD, make it into a list, and then process the list using standard python. However, note that this means that you are using only the head node to perform the computation which means that you are not getting any benefit from spark.\n",
    "\n",
    "Using RDD operations, as described below, **will** make use of all of the computers at your disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map\n",
    "* Applies a given operation to each element of an RDD.\n",
    "* Parameter is the function defining the operation.\n",
    "* Returns a new RDD.\n",
    "* Operation performed in parallel on all executors.\n",
    "* Each executor operates on teh data **local** to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Array[Int] = Array(0, 1, 4)\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.map(x => x*x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Here we are using **anonymous** functions (i.e., Lambda functions), later we will see that regular functions can also be used.\n",
    "\n",
    "For more on anonymous function see [here](https://www.scala-lang.org/old/node/133.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce\n",
    "\n",
    "* Takes RDD as input, returns a single value.\n",
    "* **Reduce operator** takes **two** elements as input returns **one** as output.\n",
    "* Repeatedly applies a **reduce operator**\n",
    "* Each executor reduces the data local to it.\n",
    "* The results from all executors are combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g., A 2-to-1 operation is the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Int = 3\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reduce((x,y)=> x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g., An example of a reduce operation that finds the shortest string in an RDD of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words: List[String] = List(this, is, the, best, mac, ever)\r\n",
       "wordRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at parallelize at <console>:26\r\n",
       "res18: String = is\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val words = List(\"this\", \"is\", \"the\", \"best\", \"mac\", \"ever\")\n",
    "val wordRDD = sc.parallelize(words)\n",
    "wordRDD.reduce((w,v)=> if (w.length()< v.length) w else v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of reduce operations\n",
    "\n",
    "* Reduce operations **must not depend on the order**\n",
    "  * Order of operands should not matter\n",
    "  * Order of application of reduce operator should not matter\n",
    "\n",
    "* Multiplication and summation are good:\n",
    "\n",
    "```\n",
    "                1 + 3 + 5 + 2                      5 + 3 + 1 + 2 \n",
    "```\n",
    "\n",
    " * Division and subtraction are bad:\n",
    "\n",
    "```\n",
    "                1 - 3 - 5 - 2                      1 - 3 - 5 - 2\n",
    "```\n",
    "\n",
    "### Why must reordering not change the result?\n",
    "\n",
    "You can think about the reduce operation as a binary tree where the leaves are the elements of the list and the root is the final result. Each triplet of the form (parent, child1, child2) corresponds to a single application of the reduce function. \n",
    "\n",
    "The order in which the reduce operation is applied is **determined at run time** and depends on how the RDD is partitioned across the cluster.\n",
    "There are many different orders to apply the reduce operation. \n",
    "\n",
    "If we want the input RDD to uniquely determine the reduced value **all evaluation orders must must yield the same final result**. In addition, the order of the elements in the list must not change the result. In particular, reversing the order of the operands in a reduce function must not change the outcome. \n",
    "\n",
    "For example the arithmetic operations multiply `*` and add `+` can be used in a reduce, but the operations subtract `-` and divide `/` should not.\n",
    "\n",
    "Doing so will not raise an error, but the result is unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at <console>:25\r\n",
       "res19: Int = -7\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val B = sc.parallelize(List(1,3,5,2))\n",
    "B.reduce((x,y)=> x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of these the following orders was executed?\n",
    "* $$((1-3)-5)-2$$ or\n",
    "* $$(1-3)-(5-2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using regular functions instead of anonymous functions\n",
    "\n",
    "Anonymous function are short and sweet but sometimes it's hard to use just one line. We can use full-fledged functions instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to find the \n",
    "* last word in a lexicographical order \n",
    "* among \n",
    "* the longest words in the list.\n",
    "\n",
    "We could achieve that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "largerThan: (x: String, y: String)String\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def largerThan(x:String, y:String): String = {\n",
    "    if (x.length() > y.length())\n",
    "       return x\n",
    "    else if (x.length() < y.length())\n",
    "       return y\n",
    "    else              //Lengths are equal, compare lexicographically\n",
    "       if (x > y)\n",
    "          return x\n",
    "       else\n",
    "          return y\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: String = this\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRDD.reduce(largerThan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We saw how to:\n",
    "* Start a SparkContext\n",
    "* Create an RDD\n",
    "* Perform Map and Reduce operations on an RDD\n",
    "* Collect the final results back to head node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining\n",
    "\n",
    "We can **chain** transformations and action to create a computation **pipeline**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to compute the sum of the squares\n",
    "$$ \\sum_{i=1}^n x_i^2 $$\n",
    "where the elements $x_i$ are stored in an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at <console>:27\r\n",
       "res24: Array[Int] = Array(0, 1, 2, 3)\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val B = sc.parallelize(List.range(0, 4))\n",
    "B.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential syntax for chaining\n",
    "\n",
    "Perform assignment after each computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "squares: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[11] at map at <console>:26\r\n",
       "res27: Int = 14\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val squares = B.map(x=>x*x)\n",
    "squares.reduce((x,y)=>(x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade syntax for chaining\n",
    "\n",
    "Combine computations into a single cascade command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: Int = 14\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.map(x=>x*x).reduce((x,y)=>x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Both syntaxes mean exactly the same thing**\n",
    "\n",
    "The only difference:\n",
    "* In the sequential syntax the intermediate RDD has a name `Squares`\n",
    "* In the cascaded syntax the intermediate RDD is *anonymous*\n",
    "\n",
    "The execution is identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential execution\n",
    "The standard way that the map and reduce are executed is\n",
    "* perform the map\n",
    "* store the resulting RDD in memory\n",
    "* perform the reduce\n",
    "\n",
    "### Disadvantages of Sequential execution\n",
    "\n",
    "1. Intermediate result (`Squares`) requires memory space.\n",
    "2. Two scans of memory (of `B`, then of `Squares`) - double the cache-misses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelined execution\n",
    "Perform the whole computation in a single pass. For each element of **`B`**\n",
    "1. Compute the square\n",
    "2. Enter the square as input to the `reduce` operation.\n",
    "\n",
    "### Advantages of Pipelined execution\n",
    "\n",
    "1. Less memory required - intermediate result is not stored.\n",
    "2. Faster - only one pass through the Input RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Evaluation\n",
    "This type of pipelined evaluation is related to **Lazy Evaluation**. The word **Lazy** is used because the first command (computing the square) is not executed immediately. Instead, the execution is delayed as long as possible so that several commands are executed in a single pass.\n",
    "\n",
    "The delayed commands are organized in an **Execution plan**.\n",
    "\n",
    "For more on Pipelined execution, Lazy evaluation and Execution Plans see [spark programming guide/RDD operations](http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations)\n",
    "\n",
    "### An instructive mistake\n",
    "Here is another way to compute the sum of the squares using a single reduce command. Can you figure out how it comes up with this unexpected result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:27\r\n",
       "res29: Int = 8\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val C = sc.parallelize(List(1,1,2))\n",
    "C.reduce((x,y)=> x*x+y*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting information about an RDD\n",
    "RDD's typically have hundreds of thousands of elements. It usually makes no sense to print out the content of a whole RDD. Here are some ways to get manageable amounts of information about an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD of length **`n`** which is a repetition of the pattern `1,2,3,4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res61: List[Int] = List(1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4)\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List.fill(3)(List(1,2,3,4)).flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n: Int = 1000000\r\n",
       "B: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[29] at parallelize at <console>:28\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val n:Int = 1000000\n",
    "val B = sc.parallelize(List.fill(n/4)(List(1,2,3,4)).flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res72: Long = 1000000\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element = 1\n",
      "First 5 elements = [1, 2, 3, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "// Get first few elements of an RDD\n",
    "println(f\"First element = ${B.first()}\")\n",
    "println(f\"First 5 elements = [${B.take(5).mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res76: Array[Int] = Array(1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3)\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling an RDD\n",
    "* RDDs are often very large.\n",
    "* Aggregates, such as averages, can be approximated efficiently by using a sample.\n",
    "* Sampling is done in parallel and requires limited computation.\n",
    "\n",
    "The method `RDD.sample(withReplacement,p)` generates a sample of the elements of the RDD. where\n",
    "- `withReplacement` is a boolean flag indicating whether or not a an element in the RDD can be sampled more than once.\n",
    "- `p` is the probability of accepting each element into the sample. Note that as the sampling is performed independently in each partition, the number of elements in the sample changes from sample to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1 = [1, 1, 3]\n",
      "sample2 = [3, 1, 2, 4, 4, 3, 3, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "m: Int = 5\r\n",
       "p: Float = 5.0E-6\n"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val m:Int = 5\n",
    "val p:Float = (m.toFloat/n.toFloat)\n",
    "//B.sample(false, p).collect()\n",
    "\n",
    "println(f\"sample1 = [${B.sample(false,p).collect().mkString(\", \")}]\")\n",
    "println(f\"sample2 = [${B.sample(false,p).collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Things to note and think about**\n",
    "\n",
    "* Each time you run the previous cell, you get a different estimate\n",
    "* The accuracy of the estimate is determined by the size of the sample $n*p$\n",
    "* See how the error changes as you vary $p$\n",
    "* Can you give a formula that relates the variance of the estimate to $(p*n)$ ? (The answer is in the Probability and statistics course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering an RDD\n",
    "The method `RDD.filter(func)` Return a new dataset formed by selecting those elements of the source on which func returns true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of elements in B that are > 3 =250000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "n_elements: Long = 250000\n"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val n_elements:Long=B.filter(n=> n>3).count()\n",
    "println(f\"The number of elements in B that are > 3 =${n_elements}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing duplicate elements from an RDD\n",
    "The method `RDD.distinct()` Returns a new dataset that contains the distinct elements of the source dataset.\n",
    "\n",
    "This operation requires a **shuffle** in order to detect duplication across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicateRDD = [1, 1, 2, 2, 3, 3]\n",
      "distinctRDD = [1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "duplicateRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[54] at parallelize at <console>:28\n"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove duplicate element in DuplicateRDD we get distinct RDD\n",
    "val duplicateRDD = sc.parallelize(List(1,1,2,2,3,3))\n",
    "println(f\"duplicateRDD = [${duplicateRDD.collect().mkString(\", \")}]\")\n",
    "println(f\"distinctRDD = [${duplicateRDD.distinct().collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatmap an RDD\n",
    "The method `RDD.flatMap(func)` is similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: List[String] = List(you are my sunshine, my only sunshine)\r\n",
       "text_file: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[81] at parallelize at <console>:29\r\n",
       "res114: Array[String] = Array(you, are, my, sunshine, my, only, sunshine)\n"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val text = List(\"you are my sunshine\", \"my only sunshine\")\n",
    "val text_file = sc.parallelize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res115: Array[Array[String]] = Array(Array(you, are, my, sunshine), Array(my, only, sunshine))\n"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Map\n",
    "text_file.map(line=>line.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res116: Array[String] = Array(you, are, my, sunshine, my, only, sunshine)\n"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Flatmap\n",
    "text_file.flatMap(line=>line.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set operations\n",
    "In this part, we explore set operations including **union**,**intersection**,**subtract**, **cartesian** in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Any] = ParallelCollectionRDD[94] at parallelize at <console>:25\r\n",
       "rdd2: org.apache.spark.rdd.RDD[Any] = ParallelCollectionRDD[95] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List[Any](1,1,2,3))\n",
    "val rdd2 = sc.parallelize(List[Any](1,3,4,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `union(other)`\n",
    " * Return the union of this RDD and another one.\n",
    " * Note that that repetitions are allowed. The RDDs are **bags** not **sets**\n",
    " * To make the result a set, use `.distinct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 = [1, 1, 2, 3]\n",
      "rdd2 = [a, b, 1]\n",
      "Union as bags = 1, 1, 2, 3, a, b, 1\n",
      "Union as sets = a, 1, b, 2, 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd2: org.apache.spark.rdd.RDD[Any] = ParallelCollectionRDD[102] at parallelize at <console>:29\n"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = sc.parallelize(List[Any](\"a\",\"b\", 1))\n",
    "\n",
    "println(f\"rdd1 = [${rdd1.collect().mkString(\", \")}]\")\n",
    "println(f\"rdd2 = [${rdd2.collect().mkString(\", \")}]\")\n",
    "\n",
    "println(f\"Union as bags = ${rdd1.union(rdd2).collect().mkString(\", \")}\")\n",
    "println(f\"Union as sets = ${rdd1.union(rdd2).distinct().collect().mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `intersection(other)`\n",
    " * Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.Note that this method performs a shuffle internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 = [1, 1, 2, 3]\n",
      "rdd2 = [1, 1, 2, 5]\n",
      "Intersection = [1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd2: org.apache.spark.rdd.RDD[Any] = ParallelCollectionRDD[108] at parallelize at <console>:29\n"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = sc.parallelize(List[Any](1,1,2,5))\n",
    "\n",
    "println(f\"rdd1 = [${rdd1.collect().mkString(\", \")}]\")\n",
    "println(f\"rdd2 = [${rdd2.collect().mkString(\", \")}]\")\n",
    "\n",
    "println(f\"Intersection = [${rdd1.intersection(rdd2).collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `subtract(other, numPartitions=None)`\n",
    " * Return each value in self that is not contained in other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 = [1, 1, 2, 3]\n",
      "rdd2 = [1, 1, 2, 5]\n",
      "Subtract = [3]\n"
     ]
    }
   ],
   "source": [
    "println(f\"rdd1 = [${rdd1.collect().mkString(\", \")}]\")\n",
    "println(f\"rdd2 = [${rdd2.collect().mkString(\", \")}]\")\n",
    "\n",
    "println(f\"Subtract = [${rdd1.subtract(rdd2).collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `cartesian(other)`\n",
    " * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where **a** is in **self** and **b** is in **other**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 = [1, 1, 2]\n",
      "rdd2 = [a, b]\n",
      "Subtract = [(1,a), (1,b), (1,a), (1,b), (2,a), (2,b)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Any] = ParallelCollectionRDD[123] at parallelize at <console>:29\r\n",
       "rdd2: org.apache.spark.rdd.RDD[Any] = ParallelCollectionRDD[124] at parallelize at <console>:30\n"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List[Any](1,1,2))\n",
    "val rdd2 = sc.parallelize(List[Any](\"a\",\"b\"))\n",
    "\n",
    "println(f\"rdd1 = [${rdd1.collect().mkString(\", \")}]\")\n",
    "println(f\"rdd2 = [${rdd2.collect().mkString(\", \")}]\")\n",
    "println(f\"Subtract = [${rdd1.cartesian(rdd2).collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "* Chaining: creating a pipeline of RDD operations.\n",
    "* counting, taking and sampling an RDD\n",
    "* More Transformations: `filter, distinct, flatmap`\n",
    "* Set transformations: `union, intersection, subtract, cartesian`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
