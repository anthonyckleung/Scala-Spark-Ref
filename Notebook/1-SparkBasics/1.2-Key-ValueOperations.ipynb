{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key-Value RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.19:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1588687919267)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@af33f8d\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Spark operations\n",
    "\n",
    "1. **Transformations**: RDD $\\rightarrow$ RDD\n",
    "  * Examples: `map`, `filter`, `sample`, and [More](http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)\n",
    "  * No communication needed\n",
    " \n",
    "\n",
    "2. **Actions**: RDD $\\rightarrow$ Python-object in head node.\n",
    "  * Examples: `reduce`, `collect`, `count`, `take`, and [More](http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions).\n",
    "  * *Some* communication needed.\n",
    "  \n",
    "  \n",
    "3. **Shuffles:** RDD $\\to$ RDD, **shuffle** needed\n",
    "  * Examples: sort, distinct, repartition, sortByKey, reduceByKey, join [More](http://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations)\n",
    "  * *A LOT* of communication might be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key-value pairs\n",
    "\n",
    "* Constructed using `Map()` constructor.\n",
    "* The **key** is used to find a set of pairs with the particular key.\n",
    "* The **value** can be anything.\n",
    "* Spark has a set of special opeartions for *(key, value)* RDDs.\n",
    "\n",
    "\n",
    "Spark provides specific functions to deal with RDDs in which each element is a key/value pair. Key/value RDDs expose new operations (e.g. aggregating and grouping together data with the same key and grouping together two different RDDs.) Such RDDs are also called pair RDDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `(key,value)` RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1**: `parallelize` a list of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,2)\n",
      "(3,4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pair_rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:27\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair_rdd = sc.parallelize(List((1,2),(3,4)))\n",
    "pair_rdd.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2**: `map` a function that maps elements to key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,1)\n",
      "(2,4)\n",
      "(3,9)\n",
      "(4,16)\n",
      "(2,4)\n",
      "(5,25)\n",
      "(6,36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reg_rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at <console>:27\r\n",
       "pair_rdd: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[6] at map at <console>:28\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reg_rdd = sc.parallelize(List(1,2,3,4,2,5,6))\n",
    "val pair_rdd = reg_rdd.map(x => (x, x*x))\n",
    "pair_rdd.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations on (key, value) RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`reduceByKey(func)`**\n",
    "\n",
    "Apply the reduce function on the values with the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD: [(1,2), (2,4), (2,6)]\n",
      "After transformation: (1,2), (2,10)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[9] at parallelize at <console>:27\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((1,2), (2,4), (2,6)))\n",
    "\n",
    "println(f\"Original RDD: [${rdd.collect.mkString(\", \")}]\")\n",
    "println(f\"After transformation: ${rdd.reduceByKey((a,b)=> a+b).collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although it is similar to the reduce function, it is implemented as a transformation and not as an action because the dataset can have very large number of keys. So, it does not return values to the driver program. Instead, it returns a new RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`sortByKey()`**\n",
    "\n",
    "Sort RDD by keys in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD: [(2,2), (1,4), (3,6)]\n",
      "After transformation: [(1,4), (2,2), (3,6)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[12] at parallelize at <console>:27\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((2,2),(1,4),(3,6)))\n",
    "\n",
    "println(f\"Original RDD: [${rdd.collect().mkString(\", \")}]\")\n",
    "println(f\"After transformation: [${rdd.sortByKey().collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The output of sortByKey() is an RDD. This means that  RDDs do have a meaningful order, which extends between partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupByKey(): \n",
    "\n",
    "Returns a new RDD of `(key,<iterator>)` pairs where the iterator iterates over the values associated with the key.\n",
    "\n",
    "[Iterators](http://anandology.com/python-practice-book/iterators.html) are python objects that generate a sequence of values. Writing a loop over `n` elements as \n",
    "```scala\n",
    "for(w <- range){\n",
    "    //do something\n",
    "}\n",
    "```\n",
    "is inefficient because it first allocates a list of `n` elements and then iterates over it.\n",
    "Using the iterator `xrange(n)` achieves the same result without materializing the list. Instead, elements are generated on the fly.\n",
    "\n",
    "To materialize the list of values returned by an iterator we will use the list comprehension command:\n",
    "```scala\n",
    "for {a in <iterator>} yield a\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD: [(1,2), (2,4), (2,6)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[21] at parallelize at <console>:27\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((1,2), (2,4), (2,6)))\n",
    "\n",
    "println(f\"Original RDD: [${rdd.collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Array[(Int, Iterable[Int])] = Array((1,List(2)), (2,List(4, 6)))\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//After transformation\n",
    "rdd.groupByKey().mapValues(x => for (a <-x) yield a).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  `flatMapValues(func)` \n",
    "\n",
    "Similar to `flatMap()`: creates a separate key/value pair for each element of the list generated by the map operation.\n",
    "\n",
    "\n",
    "`func` is a function that takes as input a single value and returns an iterator that generates a sequence of values.\n",
    "The application of flatMapValues operates on a key/value RDD. It applies `func` to each value, and gets an list (generated by the iterator) of values. It then combines each of the values with the original key to produce a list of key-value pairs. These lists are concatenated as in `flatMap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[22] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((1,2),(2,4),(2,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD :[(1,2), (2,4), (2,6)]\n"
     ]
    }
   ],
   "source": [
    "println(f\"Original RDD :[${rdd.collect().mkString(\", \")}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anonmymous function here generates for each number `i`, an iterator that produces `i,i+1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transformation : [(1,2), (1,3), (2,4), (2,5), (2,6), (2,7)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "println(f\"After transformation : [${rdd.flatMapValues(x => List.range(x,x+2)).collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Advanced)  `combineByKey(createCombiner, mergeValue, mergeCombiner)` \n",
    "Combine values with the same key using a different result type.\n",
    "\n",
    "This is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it. \n",
    "\n",
    "\n",
    "Spark's `combineByKey` method requires 3 functions:\n",
    "* `createCombiner`\n",
    "* `mergeValue`\n",
    "* `mergeCombiner`\n",
    "\n",
    "The elements of the original RDD are considered here *values*\n",
    "\n",
    "Values are converted into *combiners* which we will refer to here as \"accumulators\". An example of such a mapping is the mapping of the value *word* to the accumulator (*word*,1) that is done in WordCount.\n",
    "\n",
    "Accumulators are then combined with values and the other combiner to generate a result for each key. For example, we can use it to calculate per-activity average durations as follows. Consider an RDD of key/value pairs where keys correspond to different activities and values correspond to duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[27] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((\"Sleep\", 7), (\"Work\",5), (\"Play\", 3), \n",
    "                      (\"Sleep\", 6), (\"Work\",4), (\"Play\", 4),\n",
    "                      (\"Sleep\", 8), (\"Work\",5), (\"Play\", 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Combiner\n",
    "\n",
    "```scala\n",
    "value => (value, 1)\n",
    "```\n",
    "\n",
    "The first required argument in the `combineByKey` method is a function to be used as the very first aggregation step for each key. The argument of this function corresponds to the value in a key-value pair. If we want to compute the sum and count using `combineByKey`, then we create this \"combiner\" to be a tuple in the form of `(sum, count)`. The very first step in this aggregation is then `(value, 1)`, where `value` is the first RDD value that `combineByKey` comes across and `1` initializes the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createCombiner: Int => (Double, Int)\r\n",
       "mergeValue: ((Double, Int), Int) => (Double, Int)\r\n",
       "mergeCombiner: ((Double, Int), (Double, Int)) => (Double, Int)\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createCombiner = {(value:Int) =>\n",
    "  (value.toDouble, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge a Value\n",
    "```scala\n",
    "(accumulator, value) => (accumulator._1 + value, accumulator._2 + 1)\n",
    "```\n",
    "\n",
    "The next required function tells `combineByKey` what to do when a combiner is given a new value. The arguments to this function are a combiner and a new value. The structure of the combiner is defined above as a tuple in the form of `(sum, count)` so we merge the new value by adding it to the first element of the tuple while incrementing `1` to the second element of the tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mergeValue = {(accumulator: (Double, Int), element:Int) =>\n",
    "  (accumulator._1 + element, accumulator._2 + 1)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Combiners\n",
    "```scala\n",
    "(x, y) => (x._1 + y._1, x._2 + y._2)\n",
    "```\n",
    "\n",
    "The final required function tells `combineByKey` how to merge two combiners. In this example with tuples as combiners in the form of `(sum, count)`, all we need to do is add the first and last elements together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeCombiner = {(accumulator1: (Double, Int), accumulator2:(Double, Int)) =>\n",
    "  (accumulator1._1 + accumulator2._1, accumulator1._2 + accumulator2._2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum_counts: org.apache.spark.rdd.RDD[(String, (Double, Int))] = ShuffledRDD[30] at combineByKey at <console>:30\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sum_counts = rdd.combineByKey(createCombiner, mergeValue, mergeCombiner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Array[(String, (Double, Int))] = Array((Work,(14.0,3)), (Play,(12.0,3)), (Sleep,(21.0,3)))\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duration_means_by_activity: Array[(String, Double)] = Array((Work,4.666666666666667), (Play,4.0), (Sleep,7.0))\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val duration_means_by_activity = sum_counts.mapValues(value=> value._1*1.0/value._2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations on two (key,value) RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[32] at parallelize at <console>:25\r\n",
       "rdd2: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[33] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(List((1,2),(2,1),(2,2)))\n",
    "val rdd2 = sc.parallelize(List((2,5),(3,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `subtractByKey()`\n",
    "\n",
    "Remove from RDD1 all elements whose key is present in RDD2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 = [(1,2), (2,1), (2,2)]\n",
      "rdd2 = [(2,5), (3,1)]\n",
      "Result = [(1,2)]\n"
     ]
    }
   ],
   "source": [
    "println(f\"rdd1 = [${rdd1.collect().mkString(\", \")}]\")\n",
    "println(f\"rdd2 = [${rdd2.collect().mkString(\", \")}]\")\n",
    "println(f\"Result = [${rdd1.subtractByKey(rdd2).collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  `join()` \n",
    "* A fundamental operation in relational databases.\n",
    "* assumes two tables have a *key* column in common. \n",
    "* merges rows with the same key.\n",
    "\n",
    "Suppose we have two `(key,value)` datasets:\n",
    "\n",
    "|**dataset 1**|                                     |..........| **dataset 2** | \t       \t     |\n",
    "|-------------|-------------------------------------|   |-------------|-----------------|\n",
    "| **key=name**   |   **(gender,occupation,age)**    |   |  **key=name**   |   **hair color**    |\n",
    "| John   |  (male,cook,21)                          |   | Jill   |  blond |\n",
    "| Jill   |  (female,programmer,19)                  |   | Grace  |  brown |         \n",
    "| John   |  (male, kid, 2)                          |   | John   |  black |\n",
    "| Kate   |  (female, wrestler, 54)                  |\n",
    "\n",
    "\n",
    "When `Join` is called on datasets of type `(Key, V)` and `(Key, W)`, it  returns a dataset of `(Key, (V, W))` pairs with all pairs of elements for each key. Joining the 2 datasets above yields:\n",
    "\n",
    "\n",
    "|   key = name | (gender,occupation,age),haircolor |\n",
    "|--------------|-----------------------------------|\n",
    "| John         | ((male,cook,21),black)             |\n",
    "| John         | ((male, kid, 2),black)             |\n",
    "| Jill         | ((female,programmer,19),blond)     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1 = [(1,2), (2,1), (2,2)]\n",
      "rdd2 = [(2,5), (3,1)]\n",
      "Result = [(2,(1,5)), (2,(2,5))]\n"
     ]
    }
   ],
   "source": [
    "println(f\"rdd1 = [${rdd1.collect().mkString(\", \")}]\")\n",
    "println(f\"rdd2 = [${rdd2.collect().mkString(\", \")}]\")\n",
    "println(f\"Result = [${rdd1.join(rdd2).collect().mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants of join.\n",
    "There are four variants of `join` which differ in how they treat keys that appear in one dataset but not the other.\n",
    "* `join` is an *inner* join which means that keys that appear only in one dataset are eliminated.\n",
    "* `leftOuterJoin` keeps all keys from the left dataset even if they don't appear in the right dataset. The result of leftOuterJoin in our example will contain the keys `John, Jill, Kate`\n",
    "* `rightOuterJoin` keeps all keys from the right dataset even if they don't appear in the left dataset. The result of leftOuterJoin in our example will contain the keys `Jill, Grace, John`\n",
    "* `FullOuterJoin` keeps all keys from both datasets. The result of leftOuterJoin in our example will contain the keys `Jill, Grace, John, Kate`\n",
    "\n",
    "In outer joins, if the element appears only in one dataset, the element in `(K,(V,W))` that does not appear in the dataset is represented bye `None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions on (key, val) RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[38] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((1,2), (2,4), (2,6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `countByKey()`\n",
    "Count the number of elements for each key. Returns a dictionary for easy access to keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd = [(1,2), (2,4), (2,6)]\n",
      "Result = [1 -> 1, 2 -> 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result: scala.collection.Map[Int,Long] = Map(1 -> 1, 2 -> 2)\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(f\"rdd = [${rdd.collect().mkString(\", \")}]\")\n",
    "val result = rdd.countByKey()\n",
    "println(f\"Result = [${result.mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `collectAsMap()` \n",
    "Collect the result as a dictionary to provide easy lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd = [(1,2), (2,4), (2,6)]\n",
      "Result = [2 -> 6, 1 -> 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result: scala.collection.Map[Int,Int] = Map(2 -> 6, 1 -> 2)\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(f\"rdd = [${rdd.collect().mkString(\", \")}]\")\n",
    "val result = rdd.collectAsMap()\n",
    "println(f\"Result = [${result.mkString(\", \")}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lookup(key)` \n",
    "Return all values associated with the provided key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
