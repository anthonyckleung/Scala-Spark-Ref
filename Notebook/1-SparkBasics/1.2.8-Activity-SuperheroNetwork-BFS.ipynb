{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Superhero Popularity\n",
    "\n",
    "Task: Given a list of superhero networks, we will determine which superhero is the most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.19:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1589051986641)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2d7fd53d\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "names: org.apache.spark.rdd.RDD[String] = ../../data/marvel-names.txt MapPartitionsRDD[1] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Hero Names\n",
    "val names = sc.textFile(\"../../data/marvel-names.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \"24-HOUR MAN/EMMANUEL\"\n",
      "2 \"3-D MAN/CHARLES CHAN\"\n",
      "3 \"4-D MAN/MERCURIO\"\n",
      "4 \"8-BALL/\"\n",
      "5 \"A\"\n",
      "6 \"A'YIN\"\n",
      "7 \"ABBOTT, JACK\"\n",
      "8 \"ABCISSA\"\n",
      "9 \"ABEL\"\n",
      "10 \"ABOMINATION/EMIL BLO\"\n"
     ]
    }
   ],
   "source": [
    "// Look at a few entries\n",
    "names.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graphs: org.apache.spark.rdd.RDD[String] = ../../data/marvel-graph.txt MapPartitionsRDD[3] at textFile at <console>:26\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Hero graph\n",
    "val graphs = sc.textFile(\"../../data/marvel-graph.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5988 748 1722 3752 4655 5743 1872 3413 5527 6368 6085 4319 4728 1636 2397 3364 4001 1614 1819 1585 732 2660 3952 2507 3891 2070 2239 2602 612 1352 5447 4548 1596 5488 1605 5517 11 479 2554 2043 17 865 4292 6312 473 534 1479 6375 4456 \n",
      "5989 4080 4264 4446 3779 2430 2297 6169 3530 3272 4282 6432 2548 4140 185 105 3878 2429 1334 4595 2767 3956 3877 4776 4946 3407 128 269 5775 5121 481 5516 4758 4053 1044 1602 3889 1535 6038 533 3986 \n",
      "5982 217 595 1194 3308 2940 1815 794 1503 5197 859 5096 6039 2664 651 2244 528 284 1449 1097 1172 1092 108 3405 5204 387 4607 4545 3705 4930 1805 4712 4404 247 4754 4427 1845 536 5795 5978 533 3984 6056 \n",
      "5983 1165 3836 4361 1282 716 4289 4646 6300 5084 2397 4454 1913 5861 5485 \n",
      "5980 2731 3712 1587 6084 2472 2546 6313 875 859 323 2664 1469 522 2506 2919 2423 3624 5736 5046 1787 5776 3245 3840 2399 \n"
     ]
    }
   ],
   "source": [
    "// Look at a few entries\n",
    "graphs.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Superhero with the most co-appearances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `parseNames` extracts `hero ID -> hero name` tuples (or None in case of failure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseNames: (line: String)Option[(Int, String)]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseNames(line: String): Option[(Int, String)]={\n",
    "    var fields = line.split(\"\\\"\")\n",
    "    \n",
    "    if (fields.length >1){\n",
    "      return Some(fields(0).trim().toInt, fields(1))\n",
    "    } else{\n",
    "      return None // flatmap will just discard None results, and extract data from Some results.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `countCoOccurences` to extract the hero ID and the number of connections from each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countCoOccurences: (line: String)(Int, Int)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def countCoOccurences(line: String) = {\n",
    "    var elements = line.split(\"\\\\s+\") //Split by multiple spaces\n",
    "    (elements(0).toInt, elements.length - 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build up a `heroID -> nameRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namesRDD: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[7] at flatMap at <console>:28\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val namesRDD = names.flatMap(parseNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,24-HOUR MAN/EMMANUEL)\n",
      "(2,3-D MAN/CHARLES CHAN)\n",
      "(3,4-D MAN/MERCURIO)\n",
      "(4,8-BALL/)\n",
      "(5,A)\n"
     ]
    }
   ],
   "source": [
    "namesRDD.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to `(heroID, number of connections)` RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairings: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[4] at map at <console>:28\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairings = graphs.map(countCoOccurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5988,48)\n",
      "(5989,40)\n",
      "(5982,42)\n",
      "(5983,14)\n",
      "(5980,24)\n"
     ]
    }
   ],
   "source": [
    "pairings.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine entries that span more than one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totalFriendsByCharacter: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[5] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val totalFriendsByCharacter = pairings.reduceByKey((x,y) => x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flip it to `(# of connections, heroID)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flipped: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[6] at map at <console>:26\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flipped = totalFriendsByCharacter.map( x => (x._2, x._1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the max number of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostPopular: (Int, Int) = (1933,859)\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mostPopular = flipped.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up the name (`lookup` returns an array of results, so we need to access the first result with (0))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostPopularName: String = CAPTAIN AMERICA\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mostPopularName = namesRDD.lookup(mostPopular._2)(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degree of Separation: Breadth-First-Search\n",
    "\n",
    "**Task**: Finds the degrees of separation between two Marvel comic book characters, based on co-appearances in a comic.\n",
    "\n",
    "Here, let us represent each line from network file as a node with connections, a color, and a distance.\n",
    "\n",
    "For example:\n",
    "```\n",
    "        5983 1165 3836 4361 1282\n",
    "```\n",
    "becomes\n",
    "```\n",
    "    (5983, ((1165, 3836, 4361, 1282), 9999, WHITE))\n",
    "```\n",
    "Our initial condition is that a node is infinitely distance `(9999)` and white.\n",
    "\n",
    "Generally speaking, the *node* is of the form\n",
    "```\n",
    "    node = (characterID, (BFSData, distance, color))\n",
    "```\n",
    "\n",
    "To do this, we use *mapper* and a *reducer*:\n",
    "\n",
    "The mapper:\n",
    "* Creates new nodes for each connection of gray nodes, with a distance incremented by one, color gray, and no connections.\n",
    "* Color the gray node we just processed black.\n",
    "* Copies the node itself into the results.\n",
    "\n",
    "\n",
    "The reducer:\n",
    "* Combines together all nodes for the same hero ID.\n",
    "* Preserves the shortest distance, and the darkest color found.\n",
    "* Preserves the list of connections from the original node.\n",
    "\n",
    "Therefore, we want a breadth-first-search iteration as a map and reduce job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we know when we are done?**\n",
    "\n",
    "* An accumulator allows many executors to increment a shared variable.\n",
    "* For example:\n",
    "`var hitCounter:LongAccumulator(\"Hit Counter\")`\n",
    "sets up a shared accumulator named \"Hit Counter\" with an initial value of 0.\n",
    "* For each iteration, if the character we are interested in is hit, we increment the `hitCounter` accumulator.\n",
    "* After each iteration, we check if `hitCounter` is greater than one - if so, we are done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd._\r\n",
       "import org.apache.spark.SparkContext\r\n",
       "import org.apache.spark.util.LongAccumulator\r\n",
       "import scala.collection.mutable.ArrayBuffer\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.util.LongAccumulator\n",
    "import scala.collection.mutable.ArrayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to find the separation between these two heros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startCharacterID: Int = 5306\r\n",
       "targetCharacterID: Int = 14\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val startCharacterID = 5306 // Spiderman\n",
    "val targetCharacterID = 14 // ADAM 3031 (lol, who?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make our accumulator a \"global\" option so we can reference it in a mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hitCounter: Option[org.apache.spark.util.LongAccumulator] = None\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var hitCounter: Option[LongAccumulator] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some customer data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined type alias BFSData\r\n",
       "defined type alias BFSNode\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// BFSData - contains an array of heroID connections, the distance, and color\n",
    "type BFSData = (Array[Int], Int, String)\n",
    "\n",
    "// BFSNode has a heroID and the BFSData associated with it.\n",
    "type BFSNode = (Int, BFSData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a helper function `converToBFS` that converts each line of raw input a `BFSNode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convertToBFS: (line: String)BFSNode\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convertToBFS(line: String): BFSNode = {\n",
    "    \n",
    "    //Split up the line into fields\n",
    "    val fields = line.split(\"\\\\s+\")\n",
    "    \n",
    "    // Extract this heroID from the first field\n",
    "    val heroID = fields(0).toInt\n",
    "    \n",
    "    // Extract subsequent heroID into the connections array\n",
    "    var connections: ArrayBuffer[Int] = ArrayBuffer()     // Initialize array\n",
    "    for (connection <- 1 to (fields.length -1)){\n",
    "        connections += fields(connection).toInt\n",
    "    }\n",
    "    // Set the default distance and color as 9999 and white, respectively.\n",
    "    var color:String = \"WHITE\"\n",
    "    var distance:Int = 9999\n",
    "    \n",
    "    // Unless this is the character we are starting from\n",
    "    if (heroID == startCharacterID){\n",
    "        color = \"GRAY\"\n",
    "        distance = 0\n",
    "    }\n",
    "    \n",
    "    return (heroID, (connections.toArray, distance, color))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define another method `createStartingRDD` where it creates an \"iteration 0\" of our RDD of BFSNodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createStartingRDD: (sc: org.apache.spark.SparkContext)org.apache.spark.rdd.RDD[BFSNode]\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createStartingRDD(sc:SparkContext): RDD[BFSNode] ={\n",
    "    val inputFile = sc.textFile(\"../../data/marvel-graph.txt\")\n",
    "    return inputFile.map(convertToBFS)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a method `bfsMap` where it expands a `BFSNode` into this node and its children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bfsMap: (node: BFSNode)Array[BFSNode]\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bfsMap(node:BFSNode): Array[BFSNode] = {\n",
    "    // Extract data from the BFSNode\n",
    "    val characterID: Int = node._1\n",
    "    val data: BFSData = node._2\n",
    "    \n",
    "    val connections:Array[Int] = data._1  \n",
    "    val distance:Int = data._2\n",
    "    var color:String = data._3\n",
    "    \n",
    "    var results:ArrayBuffer[BFSNode] =ArrayBuffer()\n",
    "    //------------------\n",
    "    //-- BFS Algorithm\n",
    "    //------------------\n",
    "    // Gray nodes are flagged for expansion, and create\n",
    "    // gray nodes for each connection\n",
    "    if (color ==\"GRAY\"){\n",
    "        for (connection <- connections){\n",
    "            val newCharacterID = connection\n",
    "            val newDistance = distance + 1\n",
    "            val newColor = \"GRAY\"\n",
    "            \n",
    "            // Have we stumbled across the character we are looking for?\n",
    "            // If so increment our accumulator so the driver script knows.\n",
    "            if (targetCharacterID == connection){\n",
    "                if (hitCounter.isDefined){\n",
    "                    hitCounter.get.add(1)\n",
    "                }\n",
    "            }\n",
    "            // Create our new Gray node for this connection and add it\n",
    "            // to the results\n",
    "            val newEntry: BFSNode = (newCharacterID, (Array(), newDistance, newColor))\n",
    "            results += newEntry\n",
    "        }\n",
    "        \n",
    "        // Color this node as black, indicating it has been processed already.\n",
    "        color = \"BLACK\"\n",
    "        \n",
    "    }\n",
    "    // Add the original node back in, so its connections can get merged\n",
    "    // with the gray nodes in the reducer.\n",
    "    val thisEntry: BFSNode = (characterID, (connections, distance, color))\n",
    "    results += thisEntry\n",
    "    \n",
    "    return results.toArray\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define method `bfsReduce` that combines nodes for the same `heroID`, preserving the shortest length darket color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bfsReduce: (data1: BFSData, data2: BFSData)BFSData\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bfsReduce(data1: BFSData, data2: BFSData): BFSData = {\n",
    "    \n",
    "    // Extract data that we are combining\n",
    "    val edges1: Array[Int] = data1._1\n",
    "    val edges2: Array[Int] = data2._1\n",
    "    val distance1: Int = data1._2\n",
    "    val distance2: Int = data2._2\n",
    "    val color1:String = data1._3\n",
    "    val color2:String = data2._3\n",
    "    \n",
    "    // Default node values\n",
    "    var distance:Int = 9999\n",
    "    var color: String = \"WHITE\"\n",
    "    var edges:ArrayBuffer[Int] = ArrayBuffer()\n",
    "    \n",
    "    // See if one is the original node with its connections.\n",
    "    // If so, preserve them.\n",
    "    if (edges1.length > 0){\n",
    "        edges ++= edges1\n",
    "    }\n",
    "    if (edges2.length > 0){\n",
    "        edges ++= edges2\n",
    "    }\n",
    "    \n",
    "    // Preserve minimum distance\n",
    "    if (distance1 < distance){\n",
    "        distance = distance1\n",
    "    }\n",
    "    if (distance2 < distance){\n",
    "        distance = distance2\n",
    "    }\n",
    "    \n",
    "    //Preserve darkest color\n",
    "    if (color1 == \"WHITE\" && (color2 == \"GRAY\" || color2 == \"BLACK\")){\n",
    "        color = color2\n",
    "    }\n",
    "    if (color1 == \"GRAY\" && color2 == \"BLACK\"){\n",
    "        color = color2\n",
    "    }\n",
    "    if (color2 == \"WHITE\" && (color1 == \"GRAY\" || color1 == \"BLACK\")) {\n",
    "      color = color1\n",
    "    }\n",
    "    if (color2 == \"GRAY\" && color1 == \"BLACK\") {\n",
    "      color = color1\n",
    "    }\n",
    "    if (color1 == \"GRAY\" && color2 == \"GRAY\") {\n",
    "        color = color1\n",
    "    }\n",
    "    if (color1 == \"BLACK\" && color2 == \"BLACK\") {\n",
    "        color = color1\n",
    "    }\n",
    "    return (edges.toArray, distance, color)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform BFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define our accumulator, used to signal when we find the target character in our BFS traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hitCounter: Option[org.apache.spark.util.LongAccumulator] = Some(LongAccumulator(id: 3607, name: Some(Hit Counter), value: 0))\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hitCounter = Some(sc.longAccumulator(\"Hit Counter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.control.Breaks._\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.control.Breaks._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BFS Iteration# 1\n",
      "Processing 8330 values.\n",
      "HitCount: 30\n",
      "Hit the target character! From 30 different direction(s).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "iterationRdd: org.apache.spark.rdd.RDD[BFSNode] = MapPartitionsRDD[200] at map at <console>:48\r\n",
       "iteration: Int = 0\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "var iterationRdd = createStartingRDD(sc)\n",
    "var iteration: Int = 0\n",
    "\n",
    "breakable{\n",
    "    for (iteration <- 1 to 10){\n",
    "    println(\"Running BFS Iteration# \" + iteration)\n",
    "    \n",
    "    // Create new vertices as needed to darken or reduce distances\n",
    "    // in the reduce stage. If we encounter the node we are looking for\n",
    "    // as a GRAY node, increment our accumulator to signal that we are done.\n",
    "    val mapped = iterationRdd.flatMap(bfsMap)\n",
    "    \n",
    "    // Note that mapped.count() action here forces the RDD to be evaluated,\n",
    "    // and that is the only reason our accumulator is actually updated.\n",
    "    println(\"Processing \"+ mapped.count() + \" values.\")\n",
    "    \n",
    "    if (hitCounter.isDefined){\n",
    "        val hitCount = hitCounter.get.value\n",
    "        println(s\"HitCount: $hitCount\")\n",
    "        if (hitCount > 0){\n",
    "            println(\"Hit the target character! From \" + hitCount + \" different direction(s).\")\n",
    "            break\n",
    "        }\n",
    "    }\n",
    "    // Reducer combines data for each characterID, preserving the darkest\n",
    "    // color and shortest path.\n",
    "    iterationRdd = mapped.reduceByKey(bfsReduce)\n",
    "  }\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
