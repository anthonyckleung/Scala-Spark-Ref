{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Item-Based Collaborative Filtering\n",
    "\n",
    "**Task**: Use item-based collaborative filtering for recommending movies to users.\n",
    "\n",
    "## Approach\n",
    "* Map input ratings to `(userID, (movieID, rating))`\n",
    "* Find every movie pair rated by the same user.\n",
    "   * This can be done with a \"self-join\" operation.\n",
    "   * At this point we have `(userID, ((movieID1, rating1), (movieID2, rating2)...))`\n",
    "* Filter out duplicate pairs.\n",
    "* Make the movie pairs the key.\n",
    "   * Map to ((movieID1, movieID2), (rating1, rating2))\n",
    "* `groupByKey()` to get every rating pair found for each movie pair.\n",
    "* Compute similarity between ratings for each movie in the pair.\n",
    "* Sort, save, and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.19:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1589209136071)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6a402e0b\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\r\n",
       "import scala.io.Codec\r\n",
       "import java.nio.charset.CodingErrorAction\r\n",
       "import scala.math.sqrt\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "import scala.io.Codec\n",
    "import java.nio.charset.CodingErrorAction\n",
    "import scala.math.sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-defined Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help ease the implementation, we define a few customer data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined type alias MovieRating\r\n",
       "defined type alias UserRatingPair\r\n",
       "defined type alias RatingPair\r\n",
       "defined type alias RatingPairs\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type MovieRating = (Int, Double)\n",
    "type UserRatingPair = (Int, (MovieRating, MovieRating))\n",
    "type RatingPair = (Double, Double)\n",
    "type RatingPairs = Iterable[RatingPair]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `loadMovieNames`\n",
    "\n",
    "Simply outputs the movie names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loadMovieNames: ()Map[Int,String]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadMovieNames(): Map[Int, String] ={\n",
    "    //Handle character encoding issues\n",
    "    implicit val codec = Codec(\"UTF-8\")\n",
    "    codec.onMalformedInput(CodingErrorAction.REPLACE)\n",
    "    codec.onUnmappableCharacter(CodingErrorAction.REPLACE)\n",
    "    \n",
    "    // Create Map of Ints to Strings, and populate it from u.item\n",
    "    var movieNames:Map[Int, String] = Map()\n",
    "    var lines = Source.fromFile(\"../../ml-100k/u.item\").getLines()\n",
    "    for (line <- lines){\n",
    "        var fields = line.split('|')\n",
    "        if (fields.length > 1){\n",
    "            movieNames += (fields(0).toInt -> fields(1))\n",
    "        }\n",
    "    }\n",
    "    return movieNames\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `makePairs`\n",
    "\n",
    "Input: `UserRatingPair`\n",
    "\n",
    "Output: Tuples of the form `((movie1,movie2), (rating1,rating2))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makePairs: (userRatings: UserRatingPair)((Int, Int), (Double, Double))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makePairs(userRatings: UserRatingPair) = {\n",
    "    \n",
    "    // Extract movieRating pairs\n",
    "    val movieRating1 = userRatings._2._1\n",
    "    val movieRating2 = userRatings._2._2\n",
    "    \n",
    "    // Extract the movie and rating elements\n",
    "    val movie1 = movieRating1._1\n",
    "    val rating1 = movieRating1._2\n",
    "    \n",
    "    val movie2 = movieRating2._1\n",
    "    val rating2 = movieRating2._2\n",
    "    \n",
    "    ((movie1, movie2), (rating1, rating2))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `filterDuplicates`\n",
    "\n",
    "There may be cases where one sees \"duplicate\" entries. This function removes those duplicates.\n",
    "\n",
    "Takes a pair of user ratings and determines whether `rating_movie1 < rating_movie2`.\n",
    "\n",
    "Input: `UserRatingPair`\n",
    "\n",
    "Ouput: `Boolean`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterDuplicates: (userRatings: UserRatingPair)Boolean\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filterDuplicates(userRatings: UserRatingPair): Boolean = {\n",
    "    val movieRating1 = userRatings._2._1\n",
    "    val movieRating2 = userRatings._2._2\n",
    "    \n",
    "    val movie1 = movieRating1._1\n",
    "    val movie2 = movieRating2._1\n",
    "    \n",
    "    return movie1 < movie2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `computeCosineSimilarity`\n",
    "\n",
    "To determine whether a pair of ratings are similar, we perform a cosine similarity.\n",
    "\n",
    "Input: Pair of ratings `RatingPairs`\n",
    "Output: score `(score, numPairs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computeCosineSimilarity: (ratingPairs: RatingPairs)(Double, Int)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeCosineSimilarity(ratingPairs: RatingPairs): (Double, Int)={\n",
    "    //Initialize scores\n",
    "    var numPairs: Int = 0\n",
    "    var sum_xx: Double = 0.0\n",
    "    var sum_yy:Double = 0.0\n",
    "    var sum_xy:Double = 0.0\n",
    "    \n",
    "    // Iterate through all rating pairs\n",
    "    for (pair <- ratingPairs){\n",
    "        val ratingX = pair._1\n",
    "        val ratingY = pair._2\n",
    "        \n",
    "        sum_xx += ratingX * ratingX\n",
    "        sum_yy += ratingY * ratingY\n",
    "        sum_xy += ratingX * ratingY\n",
    "        numPairs += 1\n",
    "    }\n",
    "    val numerator:Double = sum_xy\n",
    "    val denominator = sqrt(sum_xx) * sqrt(sum_yy)\n",
    "    \n",
    "    var score:Double = 0.0\n",
    "    if (denominator != 0){\n",
    "        score = numerator/denominator\n",
    "    }\n",
    "    return (score, numPairs)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load movie names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nameDict: Map[Int,String] = Map(645 -> Paris Is Burning (1990), 892 -> Flubber (1997), 69 -> Forrest Gump (1994), 1322 -> Metisse (Caf? au Lait) (1993), 1665 -> Brother's Kiss, A (1997), 1036 -> Drop Dead Fred (1991), 1586 -> Lashou shentan (1992), 1501 -> Prisoner of the Mountains (Kavkazsky Plennik) (1996), 809 -> Rising Sun (1993), 1337 -> Larger Than Life (1996), 1411 -> Barbarella (1968), 629 -> Victor/Victoria (1982), 1024 -> Mrs. Dalloway (1997), 1469 -> Tom and Huck (1995), 365 -> Powder (1995), 1369 -> Forbidden Christ, The (Cristo proibito, Il) (1950), 138 -> D3: The Mighty Ducks (1996), 1190 -> That Old Feeling (1997), 1168 -> Little Buddha (1993), 760 -> Screamers (1995), 101 -> Heavy Metal (1981), 1454 -> Angel and the Badman (1947), 1633 -> ? k?ldum klaka (Cold Fever) (199..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nameDict = loadMovieNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[String] = ../../ml-100k/u.data MapPartitionsRDD[1] at textFile at <console>:32\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = sc.textFile(\"../../ml-100k/u.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map ratings to key/value pairs: `userID => movieID` rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings: org.apache.spark.rdd.RDD[(Int, (Int, Double))] = MapPartitionsRDD[3] at map at <console>:34\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratings = {data.map(l => l.split(\"\\t\"))\n",
    "                  .map(l => (l(0).toInt, (l(1).toInt, l(2).toDouble) ))\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emit every movie rated together by the same user. Self-join to find every combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinedRatings: org.apache.spark.rdd.RDD[(Int, ((Int, Double), (Int, Double)))] = MapPartitionsRDD[6] at join at <console>:33\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinedRatings = ratings.join(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our RDD consists of `userID => ((movieID,rating))` . We can filter out any duplicate pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniqueJoinedRatings: org.apache.spark.rdd.RDD[(Int, ((Int, Double), (Int, Double)))] = MapPartitionsRDD[7] at filter at <console>:35\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val uniqueJoinedRatings = joinedRatings.filter(filterDuplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now key by `(movie1, movie2)` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviePairs: org.apache.spark.rdd.RDD[((Int, Int), (Double, Double))] = MapPartitionsRDD[8] at map at <console>:35\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviePairs = uniqueJoinedRatings.map(makePairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have `(movie1,movie2) =>(rating1,rating2), (rating1,rating2)...`. We collect all ratings for each movie pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviePairRatings: org.apache.spark.rdd.RDD[((Int, Int), Iterable[(Double, Double)])] = ShuffledRDD[9] at groupByKey at <console>:33\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviePairRatings = moviePairs.groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviePairSimilarities: org.apache.spark.rdd.RDD[((Int, Int), (Double, Int))] = MapPartitionsRDD[10] at mapValues at <console>:35\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviePairSimilarities = moviePairRatings.mapValues(computeCosineSimilarity).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val sorted = moviePairSimilarities.sortByKey()\n",
    "//sorted.saveAsTextFile(\"movie-sims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract similarities for the movie we care about that are \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 similar movies for Star Wars (1977)\n",
      "Empire Strikes Back, The (1980)\tscore: 0.9895522078385338\tstrength: 345\n",
      "Return of the Jedi (1983)\tscore: 0.9857230861253026\tstrength: 480\n",
      "Raiders of the Lost Ark (1981)\tscore: 0.981760098872619\tstrength: 380\n",
      "20,000 Leagues Under the Sea (1954)\tscore: 0.9789385605497993\tstrength: 68\n",
      "12 Angry Men (1957)\tscore: 0.9776576120448436\tstrength: 109\n",
      "Close Shave, A (1995)\tscore: 0.9775948291054827\tstrength: 92\n",
      "African Queen, The (1951)\tscore: 0.9764692222674887\tstrength: 138\n",
      "Sting, The (1973)\tscore: 0.9751512937740359\tstrength: 204\n",
      "Wrong Trousers, The (1993)\tscore: 0.9748681355460885\tstrength: 103\n",
      "Wallace & Gromit: The Best of Aardman Animation (1996)\tscore: 0.9741816128302572\tstrength: 58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scoreThreshold: Double = 0.97\r\n",
       "coOccurenceThreshold: Double = 50.0\r\n",
       "movieID: Int = 50\r\n",
       "filteredResults: org.apache.spark.rdd.RDD[((Int, Int), (Double, Int))] = MapPartitionsRDD[11] at filter at <console>:43\r\n",
       "results: Array[((Double, Int), (Int, Int))] = Array(((0.9895522078385338,345),(50,172)), ((0.9857230861253026,480),(50,181)), ((0.981760098872619,380),(50,174)), ((0.9789385605497993,68),(50,141)), ((0.9776576120448436,109),(50,178)), ((0.9775948291054827,92),(50,408)), ((0.9764692222674887,138),(50,498)), ((0.9751512937740359,204),(50,194)), ((0.9748681355460885,103),(50,169)), ((0.9741816128302572,58),(50,114)))\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val scoreThreshold = 0.97\n",
    "val coOccurenceThreshold = 50.0\n",
    "\n",
    "// Find movies that are similar to movieID = 50\n",
    "val movieID:Int = 50\n",
    "\n",
    "//Filter for movies with this sim that are \"good\" as defined by\n",
    "// our quality threshold above.\n",
    "val filteredResults = moviePairSimilarities.filter(x=>\n",
    "                      {\n",
    "                          val pair=x._1\n",
    "                          val sim=x._2\n",
    "                          (pair._1 == movieID || pair._2 == movieID) && sim._1 > scoreThreshold && sim._2 > coOccurenceThreshold\n",
    "                      })\n",
    "\n",
    "// Sort by quality score\n",
    "val results = filteredResults.map(x=>(x._2, x._1)).sortByKey(false).take(10)\n",
    "\n",
    "println(\"\\nTop 10 similar movies for \" + nameDict(movieID))\n",
    "for (result <- results) {\n",
    "    val sim = result._1\n",
    "    val pair = result._2\n",
    "    // Display the similarity result that isn't the movie we're looking at\n",
    "    var similarMovieID = pair._1\n",
    "    if (similarMovieID == movieID) {\n",
    "        similarMovieID = pair._2\n",
    "    }\n",
    "    println(nameDict(similarMovieID) + \"\\tscore: \" + sim._1 + \"\\tstrength: \" + sim._2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
