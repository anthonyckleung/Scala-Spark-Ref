{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Min and Max Temperatures\n",
    "\n",
    "Task: find the min and max temperature by weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.math.min\r\n",
       "import scala.math.max\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math.min\n",
    "import scala.math.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@973714a\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = ../../data/1800.csv MapPartitionsRDD[9] at textFile at <console>:29\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"../../data/1800.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how each row look like. Take a sample from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554,18001231,TMIN,25,,,E,\n",
      "ITE00100554,18001231,TMAX,50,,,E,\n",
      "ITE00100554,18001230,TMIN,31,,,E,\n",
      "ITE00100554,18001230,TMAX,50,,,E,\n",
      "ITE00100554,18001229,TMIN,16,,,E,\n"
     ]
    }
   ],
   "source": [
    "lines.top(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the csv shows: `stationID, dateTime, entryTime, temperature,...`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "\n",
    "Define a method called `parseLine` that takes each row and output it in the form of `stationID, entryType, temperature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLine: (line: String)(String, String, Double)\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseLine(line: String) = {\n",
    "    val fields = line.split(\",\")     // Split the string by ','\n",
    "    val stationID = fields(0)\n",
    "    val entryType = fields(2)\n",
    "    val temp = fields(3).toFloat * 0.1f*(9.0/5.0) + 32.0f //Convert to Fahrenheit\n",
    "    (stationID, entryType, temp)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the lines with the `parseLine` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parsedLines: org.apache.spark.rdd.RDD[(String, String, Double)] = MapPartitionsRDD[19] at map at <console>:32\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsedLines = lines.map(parseLine(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out all but `TMIN` entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minTemps: org.apache.spark.rdd.RDD[(String, String, Double)] = MapPartitionsRDD[20] at filter at <console>:30\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val minTemps = parsedLines.filter(x=> x._2 == \"TMIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to `(stationID, temperature)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationMinTemps: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[21] at map at <console>:30\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stationMinTemps = minTemps.map(x => (x._1, x._3.toFloat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce by `stationID` retaining the minimum temperature found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minTempsByStation: org.apache.spark.rdd.RDD[(String, Float)] = ShuffledRDD[22] at reduceByKey at <console>:30\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val minTempsByStation = stationMinTemps.reduceByKey((x,y) => min(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect results at the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results: Array[(String, Float)] = Array((EZE00100082,7.7), (ITE00100554,5.3599997))\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = minTempsByStation.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EZE00100082 minimum temperature: 7.70 C\n",
      "ITE00100554 minimum temperature: 5.36 C\n"
     ]
    }
   ],
   "source": [
    "for (result<- results.sorted){\n",
    "    val station = result._1\n",
    "    val temp = result._2\n",
    "    val formattedTemp = f\"$temp%.2f F\"\n",
    "    println(s\"$station minimum temperature: $formattedTemp\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxTempsByStation: org.apache.spark.rdd.RDD[(String, Float)] = ShuffledRDD[26] at reduceByKey at <console>:35\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxTempsByStation = {lines.map(parseLine(_))\n",
    "                              .filter(_._2 == \"TMAX\")\n",
    "                              .map(x=>(x._1, x._3.toFloat))\n",
    "                              .reduceByKey((x,y) => max(x,y))\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxResults: Array[(String, Float)] = Array((EZE00100082,90.14), (ITE00100554,90.14))\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxResults = maxTempsByStation.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EZE00100082 maximum temperature: 90.14 F\n",
      "ITE00100554 maximum temperature: 90.14 F\n"
     ]
    }
   ],
   "source": [
    "for (result<- maxResults.sorted){\n",
    "    val station = result._1\n",
    "    val temp = result._2\n",
    "    val formattedTemp = f\"$temp%.2f F\"\n",
    "    println(s\"$station maximum temperature: $formattedTemp\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
